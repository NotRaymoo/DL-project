{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "import seaborn as sns\n",
    "\n",
    "filepath = 'diabetes_clean.csv'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeData():\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.astype(float)\n",
    "\n",
    "    color_map = {0: 'green', 1: 'red'}\n",
    "    colors = df['Outcome'].map(color_map)\n",
    "    # Create pairplot\n",
    "    sns.pairplot(df, hue='Outcome', palette=color_map)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "#visualizeData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):        \n",
    "        # Extract features and labels\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "        # Normalize features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.features = self.scaler.fit_transform(self.features)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'features': torch.tensor(self.features[idx], dtype=torch.float32),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(filepath)\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = data.iloc[:, :-1]  # Features\n",
    "y = data.iloc[:, -1]   # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork3(nn.Module):\n",
    "    def __init__(self, n_x, n_h1, n_h2, n_y):\n",
    "        # Super init\n",
    "        super(NeuralNetwork3, self).__init__()\n",
    "        \n",
    "        # Layers to use\n",
    "        self.fc1 = nn.Linear(n_x, n_h1, dtype = torch.float32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_h1, n_h2, dtype = torch.float32)\n",
    "        self.fc3 = nn.Linear(n_h2, n_y, dtype = torch.float32)\n",
    "        self.final_activation = nn.Sigmoid()\n",
    "        \n",
    "        # Loss and Accuracy metrics\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.accuracy = BinaryAccuracy() \n",
    "\n",
    "    def forward(self, x):\n",
    "        # All operations for forward, in order\n",
    "        out1 = self.fc1(x)\n",
    "        out2 = self.relu(out1)\n",
    "        out3 = self.fc2(out2)\n",
    "        out4 = self.relu(out3)\n",
    "        out5 = self.fc3(out4)\n",
    "        out6 = self.final_activation(out5)\n",
    "        return out6\n",
    "    \n",
    "class NeuralNetwork4(nn.Module):\n",
    "    def __init__(self, n_x, n_h1, n_h2, n_h3, n_y):\n",
    "        # Super init\n",
    "        super(NeuralNetwork4, self).__init__()\n",
    "        \n",
    "        # Layers to use\n",
    "        self.fc1 = nn.Linear(n_x, n_h1, dtype = torch.float32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_h1, n_h2, dtype = torch.float32)\n",
    "        self.fc3 = nn.Linear(n_h2, n_h3, dtype = torch.float32)\n",
    "        self.fc4 = nn.Linear(n_h3, n_y, dtype = torch.float32)\n",
    "        self.final_activation = nn.Sigmoid()\n",
    "        \n",
    "        # Loss and Accuracy metrics\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.accuracy = BinaryAccuracy() \n",
    "\n",
    "    def forward(self, x):\n",
    "        # All operations for forward, in order\n",
    "        out1 = self.fc1(x)\n",
    "        out2 = self.relu(out1)\n",
    "        out3 = self.fc2(out2)\n",
    "        out4 = self.relu(out3)\n",
    "        out5 = self.fc3(out4)\n",
    "        out6 = self.relu(out5)\n",
    "        out7 = self.fc4(out6)\n",
    "        out8 = self.final_activation(out7)\n",
    "        return out8\n",
    "    \n",
    "# Create Neural Network model\n",
    "n_x = 8\n",
    "n_h1 = 32\n",
    "n_h2 = 32\n",
    "n_h3 = 16\n",
    "n_y = 1\n",
    "model = NeuralNetwork3(n_x, n_h1, n_h2, n_y).to(device)\n",
    "model = NeuralNetwork4(n_x, n_h1, n_h2, n_h3, n_y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Reset index after the split\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create training and testing datasets\n",
    "dataset = CustomDataset(X_train, y_train)\n",
    "testset = CustomDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64  # Choose batch size\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/32], Training Loss: 0.1298, Training Accuracy: 0.9474\n",
      "Epoch [2/32], Training Loss: 0.0296, Training Accuracy: 0.9737\n",
      "Epoch [3/32], Training Loss: 0.0369, Training Accuracy: 1.0000\n",
      "Epoch [4/32], Training Loss: 0.0108, Training Accuracy: 1.0000\n",
      "Epoch [5/32], Training Loss: 0.1254, Training Accuracy: 0.9474\n",
      "Epoch [6/32], Training Loss: 0.0189, Training Accuracy: 1.0000\n",
      "Epoch [7/32], Training Loss: 0.0459, Training Accuracy: 0.9737\n",
      "Epoch [8/32], Training Loss: 0.0214, Training Accuracy: 1.0000\n",
      "Epoch [9/32], Training Loss: 0.0685, Training Accuracy: 0.9737\n",
      "Epoch [10/32], Training Loss: 0.0658, Training Accuracy: 0.9737\n",
      "Epoch [11/32], Training Loss: 0.1730, Training Accuracy: 0.9737\n",
      "Epoch [12/32], Training Loss: 0.0408, Training Accuracy: 0.9737\n",
      "Epoch [13/32], Training Loss: 0.2331, Training Accuracy: 0.9474\n",
      "Epoch [14/32], Training Loss: 0.0671, Training Accuracy: 0.9737\n",
      "Epoch [15/32], Training Loss: 0.4050, Training Accuracy: 0.9474\n",
      "Epoch [16/32], Training Loss: 0.0722, Training Accuracy: 0.9737\n",
      "Epoch [17/32], Training Loss: 0.0807, Training Accuracy: 0.9737\n",
      "Epoch [18/32], Training Loss: 0.1073, Training Accuracy: 0.9211\n",
      "Epoch [19/32], Training Loss: 0.0384, Training Accuracy: 0.9737\n",
      "Epoch [20/32], Training Loss: 0.0226, Training Accuracy: 1.0000\n",
      "Epoch [21/32], Training Loss: 0.1307, Training Accuracy: 0.8947\n",
      "Epoch [22/32], Training Loss: 0.0293, Training Accuracy: 1.0000\n",
      "Epoch [23/32], Training Loss: 0.0619, Training Accuracy: 0.9474\n",
      "Epoch [24/32], Training Loss: 0.0446, Training Accuracy: 1.0000\n",
      "Epoch [25/32], Training Loss: 0.0072, Training Accuracy: 1.0000\n",
      "Epoch [26/32], Training Loss: 0.0041, Training Accuracy: 1.0000\n",
      "Epoch [27/32], Training Loss: 0.0161, Training Accuracy: 1.0000\n",
      "Epoch [28/32], Training Loss: 0.0480, Training Accuracy: 0.9474\n",
      "Epoch [29/32], Training Loss: 0.0247, Training Accuracy: 0.9737\n",
      "Epoch [30/32], Training Loss: 0.0683, Training Accuracy: 0.9737\n",
      "Epoch [31/32], Training Loss: 0.0434, Training Accuracy: 1.0000\n",
      "Epoch [32/32], Training Loss: 0.0006, Training Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent parameters: optimizers, repetitions, etc.\n",
    "num_epochs = 32\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = 1e-2,\n",
    "                             betas = (0.9, 0.999),\n",
    "                             eps = 1e-08)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        inputs_re = batch['features'].to(device)\n",
    "        outputs_re = batch['label'].to(device).reshape(-1, 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(inputs_re)\n",
    "        loss_value = model.loss(pred, outputs_re)\n",
    "        # Compute binary accuracy\n",
    "        binary_accuracy_value = model.accuracy(pred, outputs_re)\n",
    "    \n",
    "        # Backward pass and optimization\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # Print loss and accuracy\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss_value.item():.4f}, Training Accuracy: {binary_accuracy_value.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.6818\n",
      "Average Loss on test set: 9.2080\n"
     ]
    }
   ],
   "source": [
    "# Define a function for evaluating the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch['features'], batch['label']\n",
    "            outputs = model(inputs)\n",
    "            loss = model.loss(outputs, labels.unsqueeze(1).float())  # Calculate the loss\n",
    "\n",
    "            # Compute accuracy\n",
    "            predicted = torch.round(outputs)  # Assuming sigmoid activation for binary classification\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "\n",
    "    print(f'Accuracy on test set: {accuracy:.4f}')\n",
    "    print(f'Average Loss on test set: {avg_loss:.4f}')\n",
    "\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
