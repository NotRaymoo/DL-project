{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC , LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier , AdaBoostClassifier ,ExtraTreesClassifier \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import ReLU ,PReLU , LeakyReLU , ELU \n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('diabetes-dataset.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def about_dataset(df):\n",
    "    print('*******SHAPE*********')\n",
    "    print('Shape of the dataset is : ',df.shape)\n",
    "    print('*******Head*********')\n",
    "    print(df.head())\n",
    "    print('*******Tail*********')\n",
    "    print(df.tail())\n",
    "    print('*******Columns in the dataframe*********')  \n",
    "    print(df.columns)\n",
    "    print('*******About the dataset*********')\n",
    "    df.info()\n",
    "    print('*******Stats on the columns of the dataframe*********')\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******SHAPE*********\n",
      "Shape of the dataset is :  (768, 9)\n",
      "*******Head*********\n",
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n",
      "*******Tail*********\n",
      "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "763           10      101             76             48      180  32.9   \n",
      "764            2      122             70             27        0  36.8   \n",
      "765            5      121             72             23      112  26.2   \n",
      "766            1      126             60              0        0  30.1   \n",
      "767            1       93             70             31        0  30.4   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  Outcome  \n",
      "763                     0.171   63        0  \n",
      "764                     0.340   27        0  \n",
      "765                     0.245   30        0  \n",
      "766                     0.349   47        1  \n",
      "767                     0.315   23        0  \n",
      "*******Columns in the dataframe*********\n",
      "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
      "      dtype='object')\n",
      "*******About the dataset*********\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n",
      "*******Stats on the columns of the dataframe*********\n",
      "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
      "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
      "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
      "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
      "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
      "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
      "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
      "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
      "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
      "\n",
      "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
      "count  768.000000                768.000000  768.000000  768.000000  \n",
      "mean    31.992578                  0.471876   33.240885    0.348958  \n",
      "std      7.884160                  0.331329   11.760232    0.476951  \n",
      "min      0.000000                  0.078000   21.000000    0.000000  \n",
      "25%     27.300000                  0.243750   24.000000    0.000000  \n",
      "50%     32.000000                  0.372500   29.000000    0.000000  \n",
      "75%     36.600000                  0.626250   41.000000    1.000000  \n",
      "max     67.100000                  2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "about_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(['Age','Outcome'],axis = 1 )\n",
    "y = dataset['Outcome']\n",
    "X_train , X_test , y_train , y_test = train_test_split(X, y, train_size = 0.7, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_dataset = pd.DataFrame({'model':[],'accuracy':[],'precision':[],'recall':[],'f1':[]})\n",
    "\n",
    "#Creating a function \n",
    "def ML(model) :\n",
    "        #Creating a Pipeline\n",
    "        global Final_dataset\n",
    "        pipe = Pipeline(\n",
    "        [\n",
    "        ('scaler',MinMaxScaler()),\n",
    "        ('classifier',model)\n",
    "        ]\n",
    "        )\n",
    "        # Fit the pipeline to the data \n",
    "        pipe.fit(X_train , y_train)\n",
    "        # Make prediction\n",
    "        y_pred = pipe.predict(X_test)   \n",
    "       \n",
    "        #Evaluate the pipeline on the test data \n",
    "        accuracy = accuracy_score(y_test , y_pred)\n",
    "        precision = precision_score(y_test , y_pred)\n",
    "        recall = recall_score(y_test , y_pred)\n",
    "        f1 = f1_score(y_test , y_pred)\n",
    "        model_name = type(model).__name__    # get the string name of the class\n",
    "        print(model_name)\n",
    "        # Appending it in the dataframe for models comparison\n",
    "        new_row = {'model':model_name,'accuracy':accuracy , 'precision':precision,'recall':recall ,'f1': f1 }\n",
    "        print(new_row)\n",
    "        Final_dataset = pd.concat([Final_dataset , pd.DataFrame(new_row, index =[0])])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_reg_model = LogisticRegression( )\n",
    "SVC_Model = SVC()\n",
    "GaussianNB_Model = GaussianNB()\n",
    "RandomForest_Model = RandomForestClassifier()\n",
    "GradientBoosting_Model = GradientBoostingClassifier()\n",
    "AdaBoost_Model = AdaBoostClassifier()\n",
    "ExtraTreesClassifier_Model = ExtraTreesClassifier()\n",
    "XGBClassifier_Model = XGBClassifier()\n",
    "LightGBM_Model = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "{'model': 'LogisticRegression', 'accuracy': 0.7748917748917749, 'precision': 0.71875, 'recall': 0.575, 'f1': 0.6388888888888888}\n",
      "SVC\n",
      "{'model': 'SVC', 'accuracy': 0.7619047619047619, 'precision': 0.676056338028169, 'recall': 0.6, 'f1': 0.6357615894039735}\n",
      "GaussianNB\n",
      "{'model': 'GaussianNB', 'accuracy': 0.7662337662337663, 'precision': 0.6625, 'recall': 0.6625, 'f1': 0.6625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "{'model': 'RandomForestClassifier', 'accuracy': 0.7402597402597403, 'precision': 0.6351351351351351, 'recall': 0.5875, 'f1': 0.6103896103896104}\n",
      "GradientBoostingClassifier\n",
      "{'model': 'GradientBoostingClassifier', 'accuracy': 0.7316017316017316, 'precision': 0.6071428571428571, 'recall': 0.6375, 'f1': 0.6219512195121951}\n",
      "AdaBoostClassifier\n",
      "{'model': 'AdaBoostClassifier', 'accuracy': 0.7402597402597403, 'precision': 0.6282051282051282, 'recall': 0.6125, 'f1': 0.620253164556962}\n",
      "ExtraTreesClassifier\n",
      "{'model': 'ExtraTreesClassifier', 'accuracy': 0.7316017316017316, 'precision': 0.618421052631579, 'recall': 0.5875, 'f1': 0.6025641025641025}\n",
      "XGBClassifier\n",
      "{'model': 'XGBClassifier', 'accuracy': 0.7489177489177489, 'precision': 0.6341463414634146, 'recall': 0.65, 'f1': 0.6419753086419753}\n",
      "[LightGBM] [Info] Number of positive: 188, number of negative: 349\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 560\n",
      "[LightGBM] [Info] Number of data points in the train set: 537, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.350093 -> initscore=-0.618630\n",
      "[LightGBM] [Info] Start training from score -0.618630\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LGBMClassifier\n",
      "{'model': 'LGBMClassifier', 'accuracy': 0.7142857142857143, 'precision': 0.5813953488372093, 'recall': 0.625, 'f1': 0.6024096385542169}\n"
     ]
    }
   ],
   "source": [
    "models = [Log_reg_model,SVC_Model,GaussianNB_Model,RandomForest_Model,GradientBoosting_Model,AdaBoost_Model,ExtraTreesClassifier_Model,XGBClassifier_Model,LightGBM_Model]\n",
    "for model in models :\n",
    "    ML(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.774892</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.638889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.635762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.766234</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.635135</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.610390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.731602</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.6375</td>\n",
       "      <td>0.621951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.6125</td>\n",
       "      <td>0.620253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>0.731602</td>\n",
       "      <td>0.618421</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.602564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.748918</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.641975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.602410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  accuracy  precision  recall        f1\n",
       "0          LogisticRegression  0.774892   0.718750  0.5750  0.638889\n",
       "0                         SVC  0.761905   0.676056  0.6000  0.635762\n",
       "0                  GaussianNB  0.766234   0.662500  0.6625  0.662500\n",
       "0      RandomForestClassifier  0.740260   0.635135  0.5875  0.610390\n",
       "0  GradientBoostingClassifier  0.731602   0.607143  0.6375  0.621951\n",
       "0          AdaBoostClassifier  0.740260   0.628205  0.6125  0.620253\n",
       "0        ExtraTreesClassifier  0.731602   0.618421  0.5875  0.602564\n",
       "0               XGBClassifier  0.748918   0.634146  0.6500  0.641975\n",
       "0              LGBMClassifier  0.714286   0.581395  0.6250  0.602410"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a neural network using Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmc = MinMaxScaler()\n",
    "X_train = mmc.fit_transform(X_train)\n",
    "X_test = mmc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.6260 - loss: 0.9555\n",
      "Epoch 2/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6654 - loss: 0.6417\n",
      "Epoch 3/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6296 - loss: 0.6503\n",
      "Epoch 4/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7199 - loss: 0.5741\n",
      "Epoch 5/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7121 - loss: 0.5602\n",
      "Epoch 6/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7642 - loss: 0.5123\n",
      "Epoch 7/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7624 - loss: 0.5265\n",
      "Epoch 8/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6894 - loss: 0.5611\n",
      "Epoch 9/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7257 - loss: 0.5293\n",
      "Epoch 10/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7834 - loss: 0.4671\n",
      "Epoch 11/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7503 - loss: 0.5063\n",
      "Epoch 12/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7501 - loss: 0.5096\n",
      "Epoch 13/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7368 - loss: 0.5294\n",
      "Epoch 14/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7544 - loss: 0.5177\n",
      "Epoch 15/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7484 - loss: 0.5124\n",
      "Epoch 16/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7330 - loss: 0.4883\n",
      "Epoch 17/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7602 - loss: 0.4584\n",
      "Epoch 18/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7512 - loss: 0.5110\n",
      "Epoch 19/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7887 - loss: 0.4554\n",
      "Epoch 20/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7649 - loss: 0.5009\n",
      "Epoch 21/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7307 - loss: 0.5620\n",
      "Epoch 22/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7906 - loss: 0.4977\n",
      "Epoch 23/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7775 - loss: 0.4551\n",
      "Epoch 24/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7529 - loss: 0.4832\n",
      "Epoch 25/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7407 - loss: 0.5049\n"
     ]
    }
   ],
   "source": [
    "# Creating the model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'accuracy' , patience = 15, restore_best_weights = True )\n",
    "\n",
    "# Create a model using the Sequential API\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(500,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(500,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(500,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(500,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(100,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1,activation = 'sigmoid')]\n",
    "    )\n",
    "\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer= tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "model.history = model.fit(X_train , y_train , epochs = 500 ,callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'ANN(5-Hidden)(0.5Cutoff)', 'accuracy': 0.6883116883116883, 'precision': 0.5317460317460317, 'recall': 0.8375, 'f1': 0.6504854368932039}\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test , y_pred)\n",
    "precision = precision_score(y_test , y_pred)\n",
    "recall = recall_score(y_test , y_pred)\n",
    "f1 = f1_score(y_test , y_pred)\n",
    "\n",
    "\n",
    "new_row = {'model':'ANN(5-Hidden)(0.5Cutoff)','accuracy':accuracy , 'precision':precision,'recall':recall ,'f1': f1 }\n",
    "print(new_row)\n",
    "Final_dataset = pd.concat([Final_dataset , pd.DataFrame(new_row, index =[0])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "{'model': 'ANN(5-Hidden)(0.7Cutoff)', 'accuracy': 0.7359307359307359, 'precision': 0.6144578313253012, 'recall': 0.6375, 'f1': 0.6257668711656442}\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred >= 0.7).astype(int)\n",
    "accuracy = accuracy_score(y_test , y_pred)\n",
    "precision = precision_score(y_test , y_pred)\n",
    "recall = recall_score(y_test , y_pred)\n",
    "f1 = f1_score(y_test , y_pred)\n",
    "\n",
    "\n",
    "new_row = {'model':'ANN(5-Hidden)(0.7Cutoff)','accuracy':accuracy , 'precision':precision,'recall':recall ,'f1': f1 }\n",
    "print(new_row)\n",
    "Final_dataset = pd.concat([Final_dataset , pd.DataFrame(new_row, index =[0])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_models_summary = Final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.774892</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.638889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.635762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.766234</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.635135</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.610390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.731602</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.6375</td>\n",
       "      <td>0.621951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.6125</td>\n",
       "      <td>0.620253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>0.731602</td>\n",
       "      <td>0.618421</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.602564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.748918</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.641975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.602410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN(5-Hidden)(0.5Cutoff)</td>\n",
       "      <td>0.735931</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.5250</td>\n",
       "      <td>0.579310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN(5-Hidden)(0.7Cutoff)</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.270833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN(5-Hidden)(0.5Cutoff)</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.8375</td>\n",
       "      <td>0.650485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN(5-Hidden)(0.7Cutoff)</td>\n",
       "      <td>0.735931</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.6375</td>\n",
       "      <td>0.625767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  accuracy  precision  recall        f1\n",
       "0          LogisticRegression  0.774892   0.718750  0.5750  0.638889\n",
       "0                         SVC  0.761905   0.676056  0.6000  0.635762\n",
       "0                  GaussianNB  0.766234   0.662500  0.6625  0.662500\n",
       "0      RandomForestClassifier  0.740260   0.635135  0.5875  0.610390\n",
       "0  GradientBoostingClassifier  0.731602   0.607143  0.6375  0.621951\n",
       "0          AdaBoostClassifier  0.740260   0.628205  0.6125  0.620253\n",
       "0        ExtraTreesClassifier  0.731602   0.618421  0.5875  0.602564\n",
       "0               XGBClassifier  0.748918   0.634146  0.6500  0.641975\n",
       "0              LGBMClassifier  0.714286   0.581395  0.6250  0.602410\n",
       "0    ANN(5-Hidden)(0.5Cutoff)  0.735931   0.646154  0.5250  0.579310\n",
       "0    ANN(5-Hidden)(0.7Cutoff)  0.696970   0.812500  0.1625  0.270833\n",
       "0    ANN(5-Hidden)(0.5Cutoff)  0.688312   0.531746  0.8375  0.650485\n",
       "0    ANN(5-Hidden)(0.7Cutoff)  0.735931   0.614458  0.6375  0.625767"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_models_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
